from pyspark.sql import SparkSession


# âš¡ Initialiser Spark
spark = SparkSession.builder \
    .config("spark.eventLog.enabled", "true") \
    .config("spark.eventLog.dir", "file:/tmp/spark-events") \
    .getOrCreate()

    #.appName("Memory Intensive Job - 512m") \
    #.config("spark.executor.memory", "512m") \
    #.config("spark.executor.memory", "4g") \
    #.config("spark.driver.memory", "4g")\

# ğŸ”¥ GÃ©nÃ©rer un grand DataFrame artificiel (10M lignes)
data = [(i % 10000, i * 1.5) for i in range(10_000_000)]
df = spark.createDataFrame(data, ["id", "value"])

# ğŸ”„ OpÃ©ration lourde sur la mÃ©moire : `groupBy()` + `agg()`
df_grouped = df.groupBy("id").agg({"value": "sum"})

# ğŸ‹ï¸â€â™‚ï¸ Forcer la mise en mÃ©moire (cache massif)
df_grouped.cache()

# â³ DÃ©clencher lâ€™exÃ©cution complÃ¨te
df_grouped.show()

# ğŸ” Afficher l'utilisation mÃ©moire Spark
executor_memory = spark._jsc.sc().getExecutorMemoryStatus()
print("ğŸ’¾ MÃ©moire des exÃ©cutors :", executor_memory)

# ğŸš€ ArrÃªter Spark
spark.stop()
