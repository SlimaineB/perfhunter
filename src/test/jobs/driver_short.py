from pyspark.sql import SparkSession
from time import time

# üìå Cr√©ation de la session Spark
spark = SparkSession.builder \
    .appName("HeavyComputeOnExecutors") \
    .config("spark.executor.memory", "1g") \
    .config("spark.executor.cores", "2") \
    .config("spark.cores.max", "3") \
    .config("spark.eventLog.enabled", "true") \
    .config("spark.eventLog.dir", "file:/tmp/spark-events") \
    .config("spark.sql.shuffle.partitions", "10") \
    .getOrCreate()

#  Cr√©ation d'un RDD volumineux simulant un gros dataset
data = [(i, i % 100) for i in range(10_000_000)]
rdd = spark.sparkContext.parallelize(data)

#  Temps de traitement c√¥t√© Driver (tr√®s court)
start_driver_time = time()
rdd.count()  # Juste pour forcer le chargement
end_driver_time = time()
driver_time = end_driver_time - start_driver_time

#  Travail intensif sur les Ex√©cutors
start_executor_time = time()
result = rdd.mapPartitions(lambda iter: [(x[0], x[1] ** 2) for x in iter]) \
            .reduceByKey(lambda a, b: a + b)
end_executor_time = time()
executor_time = end_executor_time - start_executor_time

# R√©sum√© des temps
print(f"Temps c√¥t√© Driver : {driver_time:.2f} sec")
print(f"Temps c√¥t√© Ex√©cutors : {executor_time:.2f} sec")

#  V√©rification d'un petit aper√ßu des r√©sultats
print(result.take(5))

# üîÑ Fermeture de Spark
spark.stop()
