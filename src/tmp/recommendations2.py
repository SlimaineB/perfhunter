import requests
import pandas as pd

HISTORY_SERVER = "http://<history-server>:18080"
APP_ID = "application_1234567890"

def get_api_data(endpoint):
    """Interroge l'API REST du History Server"""
    url = f"{HISTORY_SERVER}/api/v1/applications/{APP_ID}/{endpoint}"
    response = requests.get(url)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"âš ï¸ Erreur {response.status_code}: Impossible de rÃ©cupÃ©rer {endpoint}")
        return None

def analyze_performance():
    """Analyse les mÃ©triques et gÃ©nÃ¨re des recommandations"""
    executors_data = get_api_data("executors")
    stages_data = get_api_data("stages")

    # VÃ©rification des exÃ©cutors
    cpu_utilization = []
    for executor in executors_data:
        cpu_utilization.append({
            "executorId": executor["id"],
            "cpuTime": executor.get("totalExecutorCpuTime", 0),
            "runTime": executor.get("totalExecutorRunTime", 0),
            "peakMemory": executor.get("peakExecutionMemory", 0),
            "gcTime": executor.get("totalJVMGCTime", 0)
        })

    df_executors = pd.DataFrame(cpu_utilization)

    # VÃ©rification des stages
    shuffle_data = []
    for stage in stages_data:
        shuffle_data.append({
            "stageId": stage["stageId"],
            "shuffleRemoteBytesRead": stage.get("shuffleRemoteBytesRead", 0),
            "numTasks": stage["numTasks"],
            "executorRunTime": stage["executorRunTime"]
        })

    df_stages = pd.DataFrame(shuffle_data)

    # ğŸ”„ Recommandation sur le partitionnement
    avg_remote_read = df_stages["shuffleRemoteBytesRead"].mean()
    avg_num_tasks = df_stages["numTasks"].mean()
    optimal_partitions = max(100, int(avg_num_tasks * 2 + avg_remote_read / 50000))

    # ğŸ”¥ Recommandation sur lâ€™efficacitÃ© CPU
    avg_cpu_efficiency = (df_executors["cpuTime"] / df_executors["runTime"]).mean()
    cpu_recommendation = "OK" if avg_cpu_efficiency > 0.8 else "ğŸ”§ Augmenter le parallÃ©lisme"

    # ğŸ“Œ Affichage des recommandations
    print(f"ğŸ”„ Recommandation : spark.sql.shuffle.partitions = {optimal_partitions}")
    print(f"ğŸ”¥ EfficacitÃ© CPU moyenne : {avg_cpu_efficiency:.2f} ({cpu_recommendation})")
    print(f"ğŸ“Š Temps moyen dâ€™exÃ©cution des stages : {df_stages['executorRunTime'].mean():.2f} ms")

    return df_executors, df_stages, optimal_partitions

# ExÃ©cution
df_executors, df_stages, recommended_partitions = analyze_performance()
print(df_executors)
print(df_stages)
